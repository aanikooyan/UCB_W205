{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W205 - Spring 2020 - Project 2: Tracking User Activity\n",
    "\n",
    "### By: Ali Asadi Nikooyan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The list of most important commands to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the project directory:\n",
    "```\n",
    "cd project-2-aanikooyan/\n",
    "```\n",
    "Create an _assignment_ branch (just once):\n",
    "```\n",
    "git branch assignment \n",
    "```\n",
    "Checkout the _assignment_ branch:\n",
    "```\n",
    "git checkout assignment\n",
    "```\n",
    "Download the _assessments_ (.json file) into the repo (just once):\n",
    "```\n",
    "curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp (just once)\n",
    "```\n",
    "Looking at the _assessments_:\n",
    "```\n",
    "cat assessment-attempts-20180128-121051-nested.json |sort|uniq|wc -l\n",
    "```\n",
    "Copy the .yml file from the W205 course content repo into the project 2 repo (just once):\n",
    "```\n",
    "cp ~/w205/course-content/08-Querying-Data/docker-compose.yml ~/w205/project-2-aanikooyan\n",
    "```\n",
    "Edit the copied ```.yml``` file in order to make it possible to use the Spark in Jupyter Notebook (just once):\n",
    "```\n",
    "-vi docker-compose.yml\n",
    "```\n",
    "in the editor, add the following to the end of the _Spark_ section:\n",
    "```\n",
    "    expose:\n",
    "      - \"8888\"\n",
    "    ports:\n",
    "      - \"8888:8888\"\n",
    "```\n",
    "\n",
    "Spin up the cluster and check:\n",
    "```\n",
    "docker-compose up -d\n",
    "docker-compose ps\n",
    "docker ps -a\n",
    "```\n",
    "Check Kafka (in a separate SSH window):\n",
    "```\n",
    "docker-compose logs -f kafka\n",
    "```\n",
    "Check out Hadoop:\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "```\n",
    "Create a new topic called _assessments_:\n",
    "```\n",
    "docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "```\n",
    "Use ```kafkacat``` to produce test messages to the created topic:\n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-aanikooyan/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments\"\n",
    "```\n",
    "Create symbolic link in Spark (to be able to use the Spark with Jupyter Notebook):\n",
    "```\n",
    "docker-compose exec spark bash\n",
    "```\n",
    "In ``` bash shell```:\n",
    "```sh\n",
    "df -h\n",
    "ln -s /w205 w205\n",
    "ls -l\n",
    "exit\n",
    "```\n",
    "Run Spark:\n",
    " - _Method 1_- using the spark container:\n",
    " ```\n",
    "docker-compose exec spark pyspark\n",
    "```\n",
    " - _Method 2_- using Jupyter notebook (to be used in this project):\n",
    "   - First run:\n",
    "```\n",
    "docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark\n",
    "```\n",
    "   - Then, take my token and replace 0.0.0.0 with my own external IP address, for example:\n",
    "```\n",
    "        http://34.83.17.75:8888/?token=90745ddab8c1472d99ba9b3cb6fab17fe9410d1f7f99c17e\n",
    "```\n",
    "   - Finally, open ingcognito tab  and paste the modified address into it.\n",
    "   - In the Jupyter Notebook create a new ```python3``` file called Project_2 (just once)\n",
    "   \n",
    "When done with ```Project_2``` in Jupyter Notebook:\n",
    " - ```File > Save and Checkpoint```\n",
    " - ``` File > Close and Halt```\n",
    " - Logout from the Jupyter Notebook\n",
    " \n",
    " Back in SSH window:\n",
    "  - ```Ctrl+C``` and then ```Yes``` to terminate the Spark\n",
    " \n",
    "Check out saved parquet files (in the terminal window):\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/assessments/\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/extracted_assessments/\n",
    "```\n",
    "\n",
    " Docker down: \n",
    "  ```\n",
    "  docker-compose down\n",
    "  ```\n",
    "  \n",
    " Make sure that the docker is down:\n",
    "```\n",
    "  docker-compose ps\n",
    "  docker ps -a\n",
    "  docker network ls\n",
    "```\n",
    "\n",
    "\n",
    "Save the history file:\n",
    "```\n",
    "history > history.txt\n",
    "```\n",
    "Finally, ```exit()``` from all windows and stop the AI Platform Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check PySpark is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.18.0.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f88bd107cc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.18.0.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import sys\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the structure of the _assessments_ json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "p = pprint.PrettyPrinter(indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "f = open(\"assessment-attempts-20180128-121051-nested.json\",\"r\")\n",
    "s = f.read()\n",
    "json_data = json.loads(s)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_exam_id': '37f0a30a-7464-11e6-aa92-a8667f27e5dc',\n",
      " 'certification': 'false',\n",
      " 'exam_name': 'Normal Forms and All That Jazz Master Class',\n",
      " 'keen_created_at': '1516717442.735266',\n",
      " 'keen_id': '5a6745820eb8ab00016be1f1',\n",
      " 'keen_timestamp': '1516717442.735266',\n",
      " 'max_attempts': '1.0',\n",
      " 'sequences': {'attempt': 1,\n",
      "               'counts': {'all_correct': False,\n",
      "                          'correct': 2,\n",
      "                          'incomplete': 1,\n",
      "                          'incorrect': 1,\n",
      "                          'submitted': 4,\n",
      "                          'total': 4,\n",
      "                          'unanswered': 0},\n",
      "               'id': '5b28a462-7a3b-42e0-b508-09f3906d1703',\n",
      "               'questions': [{'id': '7a2ed6d3-f492-49b3-b8aa-d080a8aad986',\n",
      "                              'options': [{'at': '2018-01-23T14:23:24.670Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': '49c574b4-5c82-4ffd-9bd1-c3358faf850d',\n",
      "                                           'submitted': 1},\n",
      "                                          {'at': '2018-01-23T14:23:25.914Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': 'f2528210-35c3-4320-acf3-9056567ea19f',\n",
      "                                           'submitted': 1},\n",
      "                                          {'checked': False,\n",
      "                                           'correct': True,\n",
      "                                           'id': 'd1bf026f-554f-4543-bdd2-54dcf105b826'}],\n",
      "                              'user_correct': False,\n",
      "                              'user_incomplete': True,\n",
      "                              'user_result': 'missed_some',\n",
      "                              'user_submitted': True},\n",
      "                             {'id': 'bbed4358-999d-4462-9596-bad5173a6ecb',\n",
      "                              'options': [{'at': '2018-01-23T14:23:30.116Z',\n",
      "                                           'checked': True,\n",
      "                                           'id': 'a35d0e80-8c49-415d-b8cb-c21a02627e2b',\n",
      "                                           'submitted': 1},\n",
      "                                          {'checked': False,\n",
      "                                           'correct': True,\n",
      "                                           'id': 'bccd6e2e-2cef-4c72-8bfa-317db0ac48bb'},\n",
      "                                          {'at': '2018-01-23T14:23:41.791Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': '7e0b639a-2ef8-4604-b7eb-5018bd81a91b',\n",
      "                                           'submitted': 1}],\n",
      "                              'user_correct': False,\n",
      "                              'user_incomplete': False,\n",
      "                              'user_result': 'incorrect',\n",
      "                              'user_submitted': True},\n",
      "                             {'id': 'e6ad8644-96b1-4617-b37b-a263dded202c',\n",
      "                              'options': [{'at': '2018-01-23T14:23:52.510Z',\n",
      "                                           'checked': False,\n",
      "                                           'id': 'a9333679-de9d-41ff-bb3d-b239d6b95732'},\n",
      "                                          {'checked': False,\n",
      "                                           'id': '85795acc-b4b1-4510-bd6e-41648a3553c9'},\n",
      "                                          {'at': '2018-01-23T14:23:54.223Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': 'c185ecdb-48fb-4edb-ae4e-0204ac7a0909',\n",
      "                                           'submitted': 1},\n",
      "                                          {'at': '2018-01-23T14:23:53.862Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': '77a66c83-d001-45cd-9a5a-6bba8eb7389e',\n",
      "                                           'submitted': 1}],\n",
      "                              'user_correct': True,\n",
      "                              'user_incomplete': False,\n",
      "                              'user_result': 'correct',\n",
      "                              'user_submitted': True},\n",
      "                             {'id': '95194331-ac43-454e-83de-ea8913067055',\n",
      "                              'options': [{'checked': False,\n",
      "                                           'id': '59b9fc4b-f239-4850-b1f9-912d1fd3ca13'},\n",
      "                                          {'checked': False,\n",
      "                                           'id': '2c29e8e8-d4a8-406e-9cdf-de28ec5890fe'},\n",
      "                                          {'checked': False,\n",
      "                                           'id': '62feee6e-9b76-4123-bd9e-c0b35126b1f1'},\n",
      "                                          {'at': '2018-01-23T14:24:00.807Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': '7f13df9c-fcbe-4424-914f-2206f106765c',\n",
      "                                           'submitted': 1}],\n",
      "                              'user_correct': True,\n",
      "                              'user_incomplete': False,\n",
      "                              'user_result': 'correct',\n",
      "                              'user_submitted': True}]},\n",
      " 'started_at': '2018-01-23T14:23:19.082Z',\n",
      " 'user_exam_id': '6d4089e4-bde5-4a22-b65f-18bce9ab79c8'}\n"
     ]
    }
   ],
   "source": [
    "# this will pretty print the json in alphabetic order which may or may not match the file order\n",
    "p.pprint(json_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unrolling the _assessments_ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Read raw messages from kafka:\n",
    "raw_assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cache the raw messages (to prevent warnings later):\n",
    "raw_assessments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the Schema of the raw messages:\n",
    "raw_assessments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cast the raw messages as strings:\n",
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))\n",
    "assessments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Write this to hdfs:\n",
    "assessments.write.parquet(\"/tmp/assessments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/session.py:351: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|           sequences|          started_at|        user_exam_id|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|Map(questions -> ...|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|Map(questions -> ...|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|Map(questions -> ...|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|Map(questions -> ...|2018-01-23T20:21:...|c0ee680e-8892-4e6...|\n",
      "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|Map(questions -> ...|2018-01-23T19:48:...|e4525b79-7904-405...|\n",
      "|8b4488de-43a5-4ff...|        false|        Learning Git| 1516740790.309757|5a67a0b6852c2a000...| 1516740790.309757|         1.0|Map(questions -> ...|2018-01-23T20:51:...|3186dafa-7acf-47e...|\n",
      "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746279.3801291|5a67b627cc80e6000...|1516746279.3801291|         1.0|Map(questions -> ...|2018-01-23T22:24:...|48d88326-36a3-4cb...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743820.305464|5a67ac8cb0a5f4000...| 1516743820.305464|         1.0|Map(questions -> ...|2018-01-23T21:43:...|bb152d6b-cada-41e...|\n",
      "|1a233da8-e6e5-48a...|        false|Intermediate Pyth...|  1516743098.56811|5a67a9ba060087000...|  1516743098.56811|         1.0|Map(questions -> ...|2018-01-23T21:31:...|70073d6f-ced5-4d0...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743764.813107|5a67ac54411aed000...| 1516743764.813107|         1.0|Map(questions -> ...|2018-01-23T21:42:...|9eb6d4d6-fd1f-4f3...|\n",
      "|4cdf9b5f-fdb7-4a4...|        false|A Practical Intro...|1516744091.3127241|5a67ad9b2ff312000...|1516744091.3127241|         1.0|Map(questions -> ...|2018-01-23T21:45:...|093f1337-7090-457...|\n",
      "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746256.5878439|5a67b610baff90000...|1516746256.5878439|         1.0|Map(questions -> ...|2018-01-23T22:24:...|0f576abb-958a-4c0...|\n",
      "|87b4b3f9-3a86-435...|        false|Introduction to M...|  1516743832.99235|5a67ac9837b82b000...|  1516743832.99235|         1.0|Map(questions -> ...|2018-01-23T21:40:...|0c18f48c-0018-450...|\n",
      "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743332.7596769|5a67aaa4f21cc2000...|1516743332.7596769|         1.0|Map(questions -> ...|2018-01-23T21:34:...|b38ac9d8-eef9-495...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743750.097306|5a67ac46f7bce8000...| 1516743750.097306|         1.0|Map(questions -> ...|2018-01-23T21:41:...|bbc9865f-88ef-42e...|\n",
      "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744410.4791961|5a67aedaf34e85000...|1516744410.4791961|         1.0|Map(questions -> ...|2018-01-23T21:51:...|8a0266df-02d7-44e...|\n",
      "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744446.3999851|5a67aefef5e149000...|1516744446.3999851|         1.0|Map(questions -> ...|2018-01-23T21:53:...|95d4edb1-533f-445...|\n",
      "|f432e2e3-7e3a-4a7...|        false|Working with Algo...| 1516744255.840405|5a67ae3f0c5f48000...| 1516744255.840405|         1.0|Map(questions -> ...|2018-01-23T21:50:...|f9bc1eff-7e54-42a...|\n",
      "|76a682de-6f0c-11e...|        false|Learning iPython ...| 1516744023.652257|5a67ad579d5057000...| 1516744023.652257|         1.0|Map(questions -> ...|2018-01-23T21:46:...|dc4b35a7-399a-4bd...|\n",
      "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743398.6451161|5a67aae6753fd6000...|1516743398.6451161|         1.0|Map(questions -> ...|2018-01-23T21:35:...|d0f8249a-597e-4e1...|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the assessment data into the dataframe:\n",
    "extracted_assessments = assessments.rdd.map(lambda x: json.loads(x.value)).toDF()\n",
    "extracted_assessments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- keen_created_at: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- keen_timestamp: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- sequences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: map (containsNull = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: boolean (valueContainsNull = true)\n",
      " |-- started_at: string (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the extracted assessments:\n",
    "extracted_assessments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Save as parquet file:\n",
    "extracted_assessments.write.parquet(\"/tmp/extracted_assessments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Use SparkSQL and create a Spark \"TempTable\":\n",
    "extracted_assessments.registerTempTable('assessments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform some Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- How many assesstments are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|total_assessments|\n",
      "+-----------------+\n",
      "|             3280|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assessments_count = spark.sql(\"select count(*) as total_assessments from assessments\")\n",
    "assessments_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - How many people took _Learning Apache Hadoop_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+\n",
      "|           exam_name|number_of_people_taken|\n",
      "+--------------------+----------------------+\n",
      "|Learning Apache H...|                    16|\n",
      "+--------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_hadoop = spark.sql(\"select exam_name, count(*) as number_of_people_taken from assessments \\\n",
    "                            where exam_name like 'Learn%Apache H%' group by exam_name\")\n",
    "learning_hadoop.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- What were the top 10 most common courses taken?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           exam_name|total|\n",
      "+--------------------+-----+\n",
      "|        Learning Git|  394|\n",
      "|Introduction to P...|  162|\n",
      "|Introduction to J...|  158|\n",
      "|Intermediate Pyth...|  158|\n",
      "|Learning to Progr...|  128|\n",
      "|Introduction to M...|  119|\n",
      "|Software Architec...|  109|\n",
      "|Beginning C# Prog...|   95|\n",
      "|    Learning Eclipse|   85|\n",
      "|Learning Apache M...|   80|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_10_exams = spark.sql(\"select distinct exam_name, count(*) as total \\\n",
    "           from assessments \\\n",
    "           group by exam_name \\\n",
    "           order by total desc \\\n",
    "           limit 10\")\n",
    "top_10_exams.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - What were the bottom 10 least common courses taken?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           exam_name|total|\n",
      "+--------------------+-----+\n",
      "|Operating Red Hat...|    1|\n",
      "|Learning to Visua...|    1|\n",
      "|Nulls, Three-valu...|    1|\n",
      "|Native Web Apps f...|    1|\n",
      "|What's New in Jav...|    2|\n",
      "|Hibernate and JPA...|    2|\n",
      "|Arduino Prototypi...|    2|\n",
      "|The Closed World ...|    2|\n",
      "|Learning Spring P...|    2|\n",
      "|Understanding the...|    2|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bottom_10_exams = spark.sql(\"select distinct exam_name, count(*) as total \\\n",
    "           from assessments \\\n",
    "           group by exam_name \\\n",
    "           order by total \\\n",
    "           limit 10\")\n",
    "bottom_10_exams.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with nested multi-value and holes in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# A custom lambda function handle holes in json data\n",
    "\n",
    "def my_lambda_sequences_count(x):\n",
    "    \n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    \n",
    "    if \"sequences\" in raw_dict and \"exam_name\" in raw_dict:\n",
    "        \n",
    "        if \"counts\" in raw_dict[\"sequences\"]:\n",
    "            \n",
    "            if  \"correct\" in raw_dict[\"sequences\"][\"counts\"] and \\\n",
    "                \"incomplete\" in raw_dict[\"sequences\"][\"counts\"] and \\\n",
    "                \"incorrect\" in raw_dict[\"sequences\"][\"counts\"] and \\\n",
    "                \"submitted\" in raw_dict[\"sequences\"][\"counts\"] and \\\n",
    "                \"unanswered\" in raw_dict[\"sequences\"][\"counts\"] and \\\n",
    "                \"total\" in raw_dict[\"sequences\"][\"counts\"]  :\n",
    "                    \n",
    "                my_dict = {\"correct\": raw_dict[\"sequences\"][\"counts\"][\"correct\"],\n",
    "                           \"incomplete\": raw_dict[\"sequences\"][\"counts\"][\"incomplete\"],\n",
    "                           \"incorrect\": raw_dict[\"sequences\"][\"counts\"][\"incorrect\"],\n",
    "                           \"submitted\": raw_dict[\"sequences\"][\"counts\"][\"submitted\"],\n",
    "                           \"unanswered\": raw_dict[\"sequences\"][\"counts\"][\"unanswered\"],\n",
    "                           \"total\": raw_dict[\"sequences\"][\"counts\"][\"total\"],\n",
    "                           \"exam_name\": raw_dict[\"exam_name\"]  }\n",
    "                my_list.append(Row(**my_dict))\n",
    "    \n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5- How was the performance of all people who took _Learning Apache Hadoop_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+\n",
      "|           exam_name|number_of_people_taken|\n",
      "+--------------------+----------------------+\n",
      "|Learning Apache H...|                    16|\n",
      "+--------------------+----------------------+\n",
      "\n",
      "+--------------------+-------+---------+----------+---------+----------+-----+\n",
      "|           exam_name|correct|incorrect|incomplete|submitted|unanswered|total|\n",
      "+--------------------+-------+---------+----------+---------+----------+-----+\n",
      "|Learning Apache H...|      4|        0|         0|        4|         0|    4|\n",
      "|Learning Apache H...|      4|        0|         0|        4|         0|    4|\n",
      "|Learning Apache H...|      4|        0|         0|        4|         0|    4|\n",
      "|Learning Apache H...|      4|        0|         0|        4|         0|    4|\n",
      "|Learning Apache H...|      4|        0|         0|        4|         0|    4|\n",
      "|Learning Apache H...|      3|        1|         0|        4|         0|    4|\n",
      "|Learning Apache H...|      3|        0|         1|        4|         0|    4|\n",
      "|Learning Apache H...|      3|        1|         0|        4|         0|    4|\n",
      "|Learning Apache H...|      3|        1|         0|        4|         0|    4|\n",
      "|Learning Apache H...|      3|        0|         1|        4|         0|    4|\n",
      "|Learning Apache H...|      3|        1|         0|        4|         0|    4|\n",
      "|Learning Apache H...|      3|        1|         0|        4|         0|    4|\n",
      "|Learning Apache H...|      3|        0|         1|        4|         0|    4|\n",
      "|Learning Apache H...|      2|        1|         1|        4|         0|    4|\n",
      "|Learning Apache H...|      2|        1|         1|        4|         0|    4|\n",
      "|Learning Apache H...|      1|        3|         0|        4|         0|    4|\n",
      "+--------------------+-------+---------+----------+---------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Due to the holes in data, the custom my_lambda_sequences_count should be used here\n",
    "course_stat = assessments.rdd.flatMap(my_lambda_sequences_count).toDF()\n",
    "course_stat.registerTempTable('cs')\n",
    "\n",
    "# Total numner of people who took the course\n",
    "spark.sql(\"select exam_name, count(*) as number_of_people_taken from cs \\\n",
    "            where exam_name like 'Learn%Apache H%' group by exam_name\").show()\n",
    "\n",
    "# The performance of individuals in the exam\n",
    "spark.sql(\"select exam_name,correct,incorrect,incomplete,submitted,unanswered, total from cs  \\\n",
    "            where exam_name like 'Learn%Apache H%' order by correct desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
